#!/usr/bin/env python3
"""
ã‚µã‚¤ãƒˆæ§‹é€ èª¿æŸ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
https://hidemiyoshi.jp/blog/ ã®æ§‹é€ ã‚’åˆ†æ
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
from collections import defaultdict

class BlogAnalyzer:
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.results = {
            'sitemap': None,
            'pagination': {},
            'resources': defaultdict(set),
            'external_resources': defaultdict(set),
            'excluded_patterns': set(),
            'article_count_estimate': 0,
            'sample_urls': []
        }
    
    def check_url(self, url):
        """URLã®å­˜åœ¨ç¢ºèª"""
        try:
            response = self.session.head(url, allow_redirects=True, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def analyze_page(self, url):
        """ãƒšãƒ¼ã‚¸ã®è©³ç´°åˆ†æ"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            return soup
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            return None
    
    def check_sitemap(self):
        """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å­˜åœ¨ç¢ºèª"""
        sitemap_urls = [
            urljoin(self.base_url, 'sitemap.xml'),
            urljoin(self.base_url, 'sitemap_index.xml'),
            urljoin(self.base_url, '../sitemap.xml'),  # ãƒ«ãƒ¼ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³
        ]
        
        for sitemap_url in sitemap_urls:
            if self.check_url(sitemap_url):
                self.results['sitemap'] = sitemap_url
                print(f"âœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç™ºè¦‹: {sitemap_url}")
                return True
        
        print("âŒ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    def analyze_main_page(self):
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®åˆ†æ"""
        soup = self.analyze_page(self.base_url)
        if not soup:
            return
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ§‹é€ ã®ç¢ºèª
        pagination_links = soup.find_all('a', class_=['page-numbers', 'pagination', 'next', 'prev'])
        for link in pagination_links:
            href = link.get('href')
            if href:
                self.results['pagination'][link.text.strip()] = urljoin(self.base_url, href)
        
        # è¨˜äº‹ãƒªãƒ³ã‚¯ã®åé›†
        article_links = []
        for link in soup.find_all('a', href=True):
            href = urljoin(self.base_url, link['href'])
            if '/blog/' in href and href != self.base_url:
                parsed = urlparse(href)
                if parsed.netloc == urlparse(self.base_url).netloc:
                    article_links.append(href)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦æœ€åˆã®10ä»¶ã‚’ä¿å­˜
        unique_articles = list(set(article_links))
        self.results['sample_urls'] = unique_articles[:10]
        self.results['article_count_estimate'] = len(unique_articles)
        
        # ãƒªã‚½ãƒ¼ã‚¹ã®åˆ†æ
        self.analyze_resources(soup)
        
        # é™¤å¤–å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
        self.check_excluded_patterns(soup)
    
    def analyze_resources(self, soup):
        """ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®åˆ†æ"""
        base_domain = urlparse(self.base_url).netloc
        
        # CSS
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                domain = urlparse(full_url).netloc
                if domain == base_domain:
                    self.results['resources']['css'].add(full_url)
                else:
                    self.results['external_resources']['css'].add(full_url)
        
        # JavaScript
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                full_url = urljoin(self.base_url, src)
                domain = urlparse(full_url).netloc
                if domain == base_domain:
                    self.results['resources']['js'].add(full_url)
                else:
                    self.results['external_resources']['js'].add(full_url)
        
        # ç”»åƒ
        for img in soup.find_all('img', src=True):
            src = img.get('src')
            if src:
                full_url = urljoin(self.base_url, src)
                domain = urlparse(full_url).netloc
                if domain == base_domain:
                    if '/wp-content/uploads/' in full_url:
                        self.results['resources']['wp_uploads'].add(full_url)
                    else:
                        self.results['resources']['images'].add(full_url)
                else:
                    self.results['external_resources']['images'].add(full_url)
        
        # å¤–éƒ¨åŸ‹ã‚è¾¼ã¿ï¼ˆiframeï¼‰
        for iframe in soup.find_all('iframe'):
            src = iframe.get('src')
            if src:
                self.results['external_resources']['embed'].add(src)
    
    def check_excluded_patterns(self, soup):
        """é™¤å¤–ã™ã¹ãURLãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª"""
        excluded_patterns = [
            '/wp-admin/',
            '/wp-login.php',
            '/feed/',
            '/xmlrpc.php',
            '/?s=',  # æ¤œç´¢
            '#comment',
            '#respond',
            '/trackback/',
            '/wp-json/'
        ]
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            for pattern in excluded_patterns:
                if pattern in href:
                    self.results['excluded_patterns'].add(pattern)
    
    def generate_report(self):
        """èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'base_url': self.base_url,
            'sitemap': self.results['sitemap'],
            'pagination': dict(self.results['pagination']),
            'article_count_estimate': self.results['article_count_estimate'],
            'sample_urls': self.results['sample_urls'][:5],
            'resources': {
                'internal': {
                    'css': list(self.results['resources']['css'])[:5],
                    'js': list(self.results['resources']['js'])[:5],
                    'wp_uploads': list(self.results['resources']['wp_uploads'])[:5],
                    'images': list(self.results['resources']['images'])[:5]
                },
                'external': {
                    'css': list(self.results['external_resources']['css'])[:5],
                    'js': list(self.results['external_resources']['js'])[:5],
                    'images': list(self.results['external_resources']['images'])[:5],
                    'embed': list(self.results['external_resources']['embed'])[:5]
                }
            },
            'excluded_patterns_found': list(self.results['excluded_patterns'])
        }
        
        return report

def main():
    print("ğŸ” ã‚µã‚¤ãƒˆæ§‹é€ èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™...")
    print("=" * 50)
    
    analyzer = BlogAnalyzer('https://hidemiyoshi.jp/blog/')
    
    # ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®ç¢ºèª
    print("\nğŸ“ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®ç¢ºèª...")
    analyzer.check_sitemap()
    
    # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®åˆ†æ
    print("\nğŸ“Š ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã®åˆ†æ...")
    analyzer.analyze_main_page()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open('site_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
    print("\n" + "=" * 50)
    print("ğŸ“‹ èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 50)
    print(f"âœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—: {report['sitemap'] or 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'}")
    print(f"âœ… æ¨å®šè¨˜äº‹æ•°: {report['article_count_estimate']}ä»¶")
    print(f"âœ… ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³: {len(report['pagination'])}ä»¶ã®ãƒªãƒ³ã‚¯ç™ºè¦‹")
    
    if report['pagination']:
        print("\nğŸ“„ ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æ§‹é€ :")
        for text, url in list(report['pagination'].items())[:5]:
            print(f"  - {text}: {url}")
    
    print("\nğŸŒ å¤–éƒ¨ãƒªã‚½ãƒ¼ã‚¹:")
    for resource_type, urls in report['resources']['external'].items():
        if urls:
            print(f"  {resource_type}: {len(urls)}ä»¶")
            for url in urls[:2]:
                print(f"    - {url}")
    
    print("\nâš ï¸ é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º:")
    for pattern in report['excluded_patterns_found']:
        print(f"  - {pattern}")
    
    print("\nâœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ site_analysis_report.json ã«ä¿å­˜ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()